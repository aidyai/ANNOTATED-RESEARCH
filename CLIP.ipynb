{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### PAPER WITH CODE; LEARNING TRANSFERABLE VISUAL MODELS FROM NATURAL LANGUAGE SUPERVISION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### NOTE: THIS IS NOT AFFLIATED WITH @openai in any way rather we are standing on the Soldiers of Giants\n",
    ">>The Source Code for this Writeup is gotten from opensource implementations like:\n",
    "        > * https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.\n",
    "        > * https://github.com/ML Foundations/open_clip. Originally MIT License, Copyright (c) 2021 OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Ipython.display import Image\n",
    "Image(filename='C:\\Users\\369 Osu\\Desktop\\ANNOTATED PAPERS\\ANOTATED PAPERS\\CLIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import OrderedDict\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Union, Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from .timm_model import TimmModel\n",
    "from .utils import freeze_batch_norm_2d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABSTRACT\n",
    "\n",
    ">> State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ABOUT CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CLIP is a Neural Networkk that efficiently learns Visual Concepts from Natural Language Supervision, it is a significant step towards flexible and practical\n",
    "zero-shot computer vision classifiers which is trained on text paired with images on the internet.\n",
    "> Given a batch of N (image, text) pairs, CLIP is trained to\n",
    "predict which of the N x N possible (image, text) pairings\n",
    "across a batch actually occurred. To do this, CLIP learns a multi-modal embedding space by jointly training an image\n",
    "encoder and text encoder to maximize the cosine similarity\n",
    "of the image and text embeddings of the N real pairs\n",
    "in the batch while minimizing the cosine similarity of the\n",
    "embeddings of the N^2  N incorrect pairings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> > CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training\n",
    "examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> CLIP offers significant benefit for tasks that have relatively\n",
    "little data given its zero-shot capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPONONET ARCHITECTURE\n",
    "> ### IMAGE ENCODER:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    ">> We consider two different architectures for the image encoder.\n",
    "For the first, we use ResNet-50 (He et al., 2016a)\n",
    "as the base architecture for the image encoder due to its\n",
    "widespread adoption and proven performance. We make several\n",
    "modifications to the original version using the ResNet-\n",
    "D improvements from He et al. (2019) and the antialiased\n",
    "rect-2 blur pooling from Zhang (2019). We also replace\n",
    "the global average pooling layer with an attention pooling\n",
    "mechanism. The attention pooling is implemented as a single\n",
    "layer of “transformer-style” multi-head QKV attention\n",
    "where the query is conditioned on the global average-pooled representation of the image. For the second architecture, we\n",
    "experiment with the recently introduced Vision Transformer\n",
    "(ViT) (Dosovitskiy et al., 2020). We closely follow their\n",
    "implementation with only the minor modification of adding\n",
    "an additional layer normalization to the combined patch\n",
    "and position embeddings before the transformer and use a\n",
    "slightly different initialization scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> FIRST ARCHITECTURE THE ResNet-50\n",
    "\n",
    "        class Bottleneck(nn.Module):\n",
    "            expansion = 4\n",
    "\n",
    "            def __init__(self, inplanes, planes, stride=1):\n",
    "                super().__init__()\n",
    "\n",
    "                # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n",
    "                self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n",
    "                self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "                self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n",
    "                self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "                self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n",
    "\n",
    "                self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n",
    "                self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "\n",
    "                self.relu = nn.ReLU(inplace=True)\n",
    "                self.downsample = None\n",
    "                self.stride = stride\n",
    "\n",
    "                if stride > 1 or inplanes != planes * Bottleneck.expansion:\n",
    "                    # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n",
    "                    self.downsample = nn.Sequential(OrderedDict([\n",
    "                        (\"-1\", nn.AvgPool2d(stride)),\n",
    "                        (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n",
    "                        (\"1\", nn.BatchNorm2d(planes * self.expansion))\n",
    "                    ]))\n",
    "\n",
    "            def forward(self, x: torch.Tensor):\n",
    "                identity = x\n",
    "\n",
    "                out = self.relu(self.bn1(self.conv1(x)))\n",
    "                out = self.relu(self.bn2(self.conv2(out)))\n",
    "                out = self.avgpool(out)\n",
    "                out = self.bn3(self.conv3(out))\n",
    "\n",
    "                if self.downsample is not None:\n",
    "                    identity = self.downsample(x)\n",
    "\n",
    "                out += identity\n",
    "                out = self.relu(out)\n",
    "                return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### THE ATTENTION MECHANISM MODIFIED\n",
    "\n",
    "    class AttentionPool2d(nn.Module):\n",
    "        def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n",
    "            super().__init__()\n",
    "            self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n",
    "            self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "            self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "            self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "            self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n",
    "            self.num_heads = num_heads\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -> (HW)NC\n",
    "            x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n",
    "            x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n",
    "            x, _ = F.multi_head_attention_forward(\n",
    "                query=x, key=x, value=x,\n",
    "                embed_dim_to_check=x.shape[-1],\n",
    "                num_heads=self.num_heads,\n",
    "                q_proj_weight=self.q_proj.weight,\n",
    "                k_proj_weight=self.k_proj.weight,\n",
    "                v_proj_weight=self.v_proj.weight,\n",
    "                in_proj_weight=None,\n",
    "                in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n",
    "                bias_k=None,\n",
    "                bias_v=None,\n",
    "                add_zero_attn=False,\n",
    "                dropout_p=0,\n",
    "                out_proj_weight=self.c_proj.weight,\n",
    "                out_proj_bias=self.c_proj.bias,\n",
    "                use_separate_proj_weight=True,\n",
    "                training=self.training,\n",
    "                need_weights=False\n",
    "            )\n",
    "\n",
    "            return x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\n",
    "\n",
    "    class ModifiedResNet(nn.Module):\n",
    "        \"\"\"\n",
    "        A ResNet class that is similar to torchvision's but contains the following changes:\n",
    "        - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n",
    "        - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n",
    "        - The final pooling layer is a QKV attention instead of an average pool\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n",
    "            super().__init__()\n",
    "            self.output_dim = output_dim\n",
    "            self.input_resolution = input_resolution\n",
    "\n",
    "            # the 3-layer stem\n",
    "            self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(width // 2)\n",
    "            self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n",
    "            self.bn2 = nn.BatchNorm2d(width // 2)\n",
    "            self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n",
    "            self.bn3 = nn.BatchNorm2d(width)\n",
    "            self.avgpool = nn.AvgPool2d(2)\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "            # residual layers\n",
    "            self._inplanes = width  # this is a *mutable* variable used during construction\n",
    "            self.layer1 = self._make_layer(width, layers[0])\n",
    "            self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n",
    "            self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n",
    "            self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n",
    "\n",
    "            embed_dim = width * 32  # the ResNet feature dimension\n",
    "            self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, heads, output_dim)\n",
    "\n",
    "        def _make_layer(self, planes, blocks, stride=1):\n",
    "            layers = [Bottleneck(self._inplanes, planes, stride)]\n",
    "\n",
    "            self._inplanes = planes * Bottleneck.expansion\n",
    "            for _ in range(1, blocks):\n",
    "                layers.append(Bottleneck(self._inplanes, planes))\n",
    "\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        def forward(self, x):\n",
    "            def stem(x):\n",
    "                for conv, bn in [(self.conv1, self.bn1), (self.conv2, self.bn2), (self.conv3, self.bn3)]:\n",
    "                    x = self.relu(bn(conv(x)))\n",
    "                x = self.avgpool(x)\n",
    "                return x\n",
    "\n",
    "            x = x.type(self.conv1.weight.dtype)\n",
    "            x = stem(x)\n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.layer4(x)\n",
    "            x = self.attnpool(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "\n",
    "    class LayerNorm(nn.LayerNorm):\n",
    "        \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
    "\n",
    "        def forward(self, x: torch.Tensor):\n",
    "            orig_type = x.dtype\n",
    "            ret = super().forward(x.type(torch.float32))\n",
    "            return ret.type(orig_type)\n",
    "\n",
    "\n",
    "    class QuickGELU(nn.Module):\n",
    "        def forward(self, x: torch.Tensor):\n",
    "            return x * torch.sigmoid(1.702 * x)\n",
    "\n",
    "\n",
    "    class ResidualAttentionBlock(nn.Module):\n",
    "        def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n",
    "            super().__init__()\n",
    "\n",
    "            self.attn = nn.MultiheadAttention(d_model, n_head)\n",
    "            self.ln_1 = LayerNorm(d_model)\n",
    "            self.mlp = nn.Sequential(OrderedDict([\n",
    "                (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
    "                (\"gelu\", QuickGELU()),\n",
    "                (\"c_proj\", nn.Linear(d_model * 4, d_model))\n",
    "            ]))\n",
    "            self.ln_2 = LayerNorm(d_model)\n",
    "            self.attn_mask = attn_mask\n",
    "\n",
    "        def attention(self, x: torch.Tensor):\n",
    "            self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n",
    "            return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n",
    "\n",
    "        def forward(self, x: torch.Tensor):\n",
    "            x = x + self.attention(self.ln_1(x))\n",
    "            x = x + self.mlp(self.ln_2(x))\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SECOND ARCHITECTURE THE VISION TRANSFORMER   \n",
    "\n",
    "    class VisionTransformer(nn.Module):\n",
    "        def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):\n",
    "            super().__init__()\n",
    "            self.input_resolution = input_resolution\n",
    "            self.output_dim = output_dim\n",
    "            self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n",
    "\n",
    "            scale = width ** -0.5\n",
    "            self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
    "            self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n",
    "            self.ln_pre = LayerNorm(width)\n",
    "            self.transformer = Transformer(width, layers, heads)\n",
    "            self.ln_post = LayerNorm(width)\n",
    "            self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n",
    "\n",
    "        def forward(self, x: torch.Tensor):\n",
    "            x = self.conv1(x)  # shape = [*, width, grid, grid]\n",
    "            x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n",
    "            x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
    "            x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "            x = x + self.positional_embedding.to(x.dtype)\n",
    "            x = self.ln_pre(x)\n",
    "\n",
    "            x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "            x = self.transformer(x)\n",
    "            x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "\n",
    "            x = self.ln_post(x[:, 0, :])\n",
    "\n",
    "            if self.proj is not None:\n",
    "                x = x @ self.proj\n",
    "\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### TEXT ENCODER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> The text encoder is a Transformer (Vaswani et al., 2017)\n",
    "with the architecture modifications described in Radford\n",
    "et al. (2019). As a base size we use a 63M-parameter 12-\n",
    "layer 512-wide model with 8 attention heads. The transformer\n",
    "operates on a lower-cased byte pair encoding (BPE)\n",
    "representation of the text with a 49,152 vocab size (Sennrich\n",
    "et al., 2015). For computational efficiency, the max\n",
    "sequence length was capped at 76. The text sequence is\n",
    "bracketed with [SOS] and [EOS] tokens and the activations\n",
    "of the highest layer of the transformer at the [EOS]\n",
    "token are treated as the feature representation of the text\n",
    "which is layer normalized and then linearly projected into\n",
    "the multi-modal embedding space. Masked self-attention\n",
    "was used in the text encoder to preserve the ability to initialize\n",
    "with a pre-trained language model or add language\n",
    "modeling as an auxiliary objective, though exploration of\n",
    "this is left as future work. For the text encoder, we\n",
    "only scale the width of the model to be proportional to the\n",
    "calculated increase in width of the ResNet and do not scale\n",
    "the depth at all, as we found CLIP’s performance to be less\n",
    "sensitive to the capacity of the text encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \n",
    "    class Transformer(nn.Module):\n",
    "            def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n",
    "                super().__init__()\n",
    "                self.width = width\n",
    "                self.layers = layers\n",
    "                self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n",
    "\n",
    "            def forward(self, x: torch.Tensor):\n",
    "                return self.resblocks(x)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## EXPERIMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this work, we close this gap and study the behaviors of image classifiers trained with natural language supervision at large scale. Enabled\n",
    "by the large amounts of publicly available data of this form\n",
    "on the internet, we create a new dataset of 400 million (image,\n",
    "text) pairs and demonstrate that a simplified version of\n",
    "ConVIRT trained from scratch, which we call CLIP, for Contrastive\n",
    "Language-Image Pre-training, is an efficient method\n",
    "of learning from natural language supervision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We find that CLIP, similar to the GPT family, learns\n",
    "to perform a wide set of tasks during pre-training including\n",
    "OCR, geo-localization, action recognition, and many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We also confirm these findings with linear-probe\n",
    "representation learning analysis and show that CLIP outperforms\n",
    "the best publicly available ImageNet model while\n",
    "also being more computationally efficient. We additionally\n",
    "find that zero-shot CLIP models are much more robust than\n",
    "equivalent accuracy supervised ImageNet models which\n",
    "suggests that zero-shot evaluation of task-agnostic models is\n",
    "much more representative of a model’s capability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> At the core of our approach is the idea of learning perception\n",
    "from supervision contained in natural language.\n",
    "\n",
    "> Learning from natural language has several potential\n",
    "strengths over other training methods. It’s much easier\n",
    "to scale natural language supervision compared to standard\n",
    "crowd-sourced labeling for image classification since it does\n",
    "not require annotations to be in a classic “machine learning\n",
    "compatible format” such as the canonical 1-of-N majority\n",
    "vote “gold label”. Instead, methods which work on natural\n",
    "language can learn passively from the supervision contained\n",
    "in the vast amount of text on the internet.\n",
    "Learning from natural language also has an important advantage over most\n",
    "unsupervised or self-supervised learning approaches in that\n",
    "it doesn’t “just” learn a representation but also connects that\n",
    "representation to language which enables flexible zero-shot\n",
    "transfer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">CLIP is pre-trained to predict if an image and a text snippet\n",
    "are paired together in its dataset. To perform zero-shot classification,\n",
    "we reuse this capability. For each dataset, we use\n",
    "the names of all the classes in the dataset as the set of potential\n",
    "text pairings and predict the most probable (image, text)\n",
    "pair according to CLIP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In a bit more detail, we first compute\n",
    "the feature embedding of the image and the feature embedding\n",
    "of the set of possible texts by their respective encoders.\n",
    "The cosine similarity of these embeddings is then calculated,\n",
    "scaled by a temperature parameter \u001c , and normalized into a\n",
    "probability distribution via a softmax. Note that this prediction\n",
    "layer is a multinomial logistic regression classifier with\n",
    "L2-normalized inputs, L2-normalized weights, no bias, and\n",
    "temperature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When interpreted this way, the image\n",
    "encoder is the computer vision backbone which computes a\n",
    "feature representation for the image and the text encoder is a\n",
    "hypernetwork (Ha et al., 2016) which generates the weights\n",
    "of a linear classifier based on the text specifying the visual\n",
    "concepts that the classes represent. Lei Ba et al. (2015) first\n",
    "introduced a zero-shot image classifier of this form while\n",
    "the idea of generating a classifier from natural language\n",
    "dates back to at least Elhoseiny et al. (2013)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ### PROMPT ENGINEERING AND EMSEMBLING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Most standard image classification datasets treat the information\n",
    "naming or describing classes which enables natural\n",
    "language based zero-shot transfer as an afterthought. The\n",
    "vast majority of datasets annotate images with just a numeric\n",
    "id of the label and contain a file mapping these ids back to\n",
    "their names in English. Using the prompt template\n",
    "“A photo of a { label }.” to be a good default that\n",
    "helps specify the text is about the content of the image. This\n",
    "often improves performance over the baseline of using only\n",
    "the label text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We also experimented with ensembling over multiple zeroshot\n",
    "classifiers as another way of improving performance.\n",
    "These classifiers are computed by using different context\n",
    "prompts such as ‘A photo of a big flabelg” and\n",
    "“A photo of a small flabelg”. We construct the\n",
    "ensemble over the embedding space instead of probability\n",
    "space. This allows us to cache a single set of averaged text\n",
    "embeddings so that the compute cost of the ensemble is the\n",
    "same as using a single classifier when amortized over many\n",
    "predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Due to the large size of our pre-training dataset, over-fitting\n",
    "is not a major concern and the details of training CLIP are\n",
    "simplified compared to the implementation of Zhang et al.\n",
    "(2020). We train CLIP from scratch without initializing the\n",
    "image encoder with ImageNet weights or the text encoder\n",
    "with pre-trained weights. We do not use the non-linear\n",
    "projection between the representation and the contrastive\n",
    "embedding space, a change which was introduced by Bachman\n",
    "et al. (2019) and popularized by Chen et al. (2020b).\n",
    "We instead use only a linear projection to map from each encoder’s\n",
    "representation to the multi-modal embedding space.\n",
    "We did not notice a difference in training efficiency between\n",
    "the two versions and speculate that non-linear projections\n",
    "may be co-adapted with details of current image only in\n",
    "self-supervised representation learning methods. We also\n",
    "remove the text transformation function tu from Zhang et al.\n",
    "(2020) which samples a single sentence at uniform from\n",
    "the text since many of the (image, text) pairs in CLIP’s pretraining\n",
    "dataset are only a single sentence. We also simplify\n",
    "the image transformation function tv. A random square\n",
    "crop from resized images is the only data augmentation\n",
    "used during training. Finally, the temperature parameter\n",
    "which controls the range of the logits in the softmax, \u001c , is\n",
    "directly optimized during training as a log-parameterized\n",
    "multiplicative scalar to avoid turning as a hyper-parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CLIP has a wide range of capabilities due to its ability to\n",
    "carry out arbitrary image classification tasks. One can give\n",
    "it images of cats and dogs and ask it to classify cats, or give\n",
    "it images taken in a department store and ask it to classify\n",
    "shoplifters–a task with significant social implications and\n",
    "for which AI may be unfit. Like any image classification\n",
    "system, CLIP’s performance and fitness for purpose need to\n",
    "be evaluated, and its broader impacts analyzed in context.\n",
    "CLIP also introduces a capability that will magnify and alter\n",
    "such issues: CLIP makes it possible to easily create your\n",
    "own classes for categorization (to ‘roll your own classifier’)\n",
    "without a need for re-training. This capability introduces\n",
    "challenges similar to those found in characterizing other,\n",
    "large-scale generative models like GPT-3 (Brown et al.,\n",
    "2020); models that exhibit non-trivial zero-shot (or fewshot)\n",
    "generalization can have a vast range of capabilities,\n",
    "many of which are made clear only after testing for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Our studies of CLIP in a zero-shot setting show that the\n",
    "model displays significant promise for widely-applicable\n",
    "tasks like image retrieval or search. For example, it can find\n",
    "relevant images in a database given text, or relevant text\n",
    "given an image. Further, the relative ease of steering CLIP\n",
    "toward bespoke applications with little or no additional data\n",
    "or training could unlock a variety of novel applications that\n",
    "are hard for us to envision today, as has occurred with large\n",
    "language models over the past few years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, CLIP does unlock a certain aspect of usability\n",
    "given how it removes the need for training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CLIP is instead focused\n",
    "on learning visual models from scratch via natural\n",
    "language supervision and does not densely connect the two\n",
    "domains with a joint attention model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "> The only interaction\n",
    "in a CLIP model between the image and text domain is a\n",
    "single dot product in a learned joint embedding space."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c347d5b110094c3c84b38995bb070df963df71643c99aeb6475918bc71b4c228"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 ('AIDY.ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
